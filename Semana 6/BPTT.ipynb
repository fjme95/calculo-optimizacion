{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BPTT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUcsinbS/iEAv43ZLRKBZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjme95/calculo-optimizacion/blob/main/Semana%206/BPTT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todo el código viene de https://github.com/devnag/tensorflow-bptt\n",
        "\n",
        "Post en medium: https://medium.com/@devnag/a-simple-design-pattern-for-recurrent-deep-learning-in-tensorflow-37aba4e2fd6b"
      ],
      "metadata": {
        "id": "SD4p_1M5dX52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencias\n"
      ],
      "metadata": {
        "id": "FXsgWoYLceMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv1HbgZ4vTGR",
        "outputId": "50c36923-4c4a-4ad6-9d8b-af96625b7672"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dinoCYAGb7lP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "import math\n",
        "import random\n",
        "import sys\n",
        "\n",
        "# See https://medium.com/@devnag/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation Through Time"
      ],
      "metadata": {
        "id": "bivF-mopch1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BPTT(object):\n",
        "    \"\"\"\n",
        "    Convenience design pattern for handling simple recurrent graphs, implementing backpropagation through time.\n",
        "    See https://medium.com/@devnag/\n",
        "    Typical usage:\n",
        "    - Graph building\n",
        "        - Define a function that takes a BPTT object and the depth flag (will be BPTT.DEEP or BPTT.SHALLOW)\n",
        "              and builds your computational graph; should return any I/O placeholders in an array.\n",
        "          - Use get_past_variable() to define a name (string) and pass in a constant value (numpy).\n",
        "          - Use name_variable() to name (string) the same value for the current loop, for the future.\n",
        "    - Unrolling\n",
        "        - bp.generate_graphs() will take the function above and the desired BPTT depth and provide the\n",
        "            sequence of stitched DAGs.\n",
        "    - Training\n",
        "        - generate_feed_dict() on the relevant depth (BPTT.DEEP) with the array data to be fed into the\n",
        "           I/O placeholders that your custom graph function returned. This will also include the working\n",
        "           state for the recurrent variables (whether the starting constants or state from the last loop).\n",
        "           Must also include a count of the number of I/O slots.\n",
        "        - generate_output_definitions() will provide an array of variables that must be fetched to extract state.\n",
        "        - save_output_state() will take the results and save for the next loop.\n",
        "    - Inference\n",
        "        - Same three functions as in training, but use BPTT.SHALLOW instead.\n",
        "        - Can optionally call copy_state_forward() before inference if you want to start with the final training state.\n",
        "    \"\"\"\n",
        "\n",
        "    DEEP = \"deep\"\n",
        "    SHALLOW = \"shallow\"\n",
        "    MODEL_NAME = \"unrolled_model\"\n",
        "    LOOP_SCOPE = \"unroll\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the name dictionaries (state, placeholders, constants, etc)\n",
        "        \"\"\"\n",
        "        self.graph_dict = {}\n",
        "\n",
        "        # Name -> Constants: Starting values (typically np.arrays). Shared between shallow/deep, used in run-time\n",
        "        self.starting_constants = {}\n",
        "        # Name -> State: np.arrays reflecting state between run-times (starting from C)\n",
        "        self.state = {self.DEEP: {}, self.SHALLOW: {}}\n",
        "        # Name -> Variables: Py variables passed through during build-time\n",
        "        self.vars = {self.DEEP: {}, self.SHALLOW: {}}\n",
        "        # Name -> Placeholder: Placeholders: to inject state, set during build-time\n",
        "        self.placeholders = {self.DEEP: {}, self.SHALLOW: {}}\n",
        "\n",
        "        self.current_depth = self.DEEP\n",
        "\n",
        "    def get_past_variable(self, variable_name, starting_value):\n",
        "        \"\"\"\n",
        "        Get-or-set a recurrent variable from the past (time t-1)\n",
        "        :param variable_name: A unique (to this object) string representing this variable.\n",
        "        :param starting_value: A constant that can be fed into a placeholder eventually\n",
        "        :return: A variable (representing the value at t-1) that can be computed on to generate current value (at t)\n",
        "        \"\"\"\n",
        "\n",
        "        if variable_name not in self.placeholders[self.current_depth]:\n",
        "            # First time being called\n",
        "            self.starting_constants[variable_name] = starting_value\n",
        "\n",
        "            # First initial state is the constant np.array sent in\n",
        "            self.state[self.current_depth][variable_name] = starting_value\n",
        "\n",
        "            # Define a mirror placeholder with same type/shape\n",
        "            self.placeholders[self.current_depth][variable_name] = tf.placeholder(starting_value.dtype,\n",
        "                                                                                  shape=starting_value.shape)\n",
        "            # Set current (starting) variable as that placeholder, to be filled in later\n",
        "            self.vars[self.current_depth][variable_name] = self.placeholders[self.current_depth][variable_name]\n",
        "\n",
        "        # Return the pyvariable: placeholder the first time, pydescendant on later calls\n",
        "        return self.vars[self.current_depth][variable_name]\n",
        "\n",
        "    def name_variable(self, variable_name, v):\n",
        "        \"\"\"\n",
        "        Set/assign a recurrent variable for the current time (time t)\n",
        "        :param variable_name: A unique (to this object) string, must have been used in a get_past_variable() call\n",
        "        :param v: A Tensorflow variable representing the current value of this variable (at t)\n",
        "        :return: v, unchanged, for easy in-line usage\n",
        "        \"\"\"\n",
        "        assert variable_name in self.vars[self.current_depth], \\\n",
        "            \"Tried to set variable name that was never defined with get_past_variable()\"\n",
        "        self.vars[self.current_depth][variable_name] = v\n",
        "        return v\n",
        "\n",
        "    def generate_graphs(self, func, num_loops=10):\n",
        "        \"\"\"\n",
        "        Generate the two graphs -- the deep (unrolled) connected graphs and the shallow/simple graph.\n",
        "        :param func: A function which takes the BPTT object and the depth_type (BPTT.{DEEP,SHALLOW}), returns\n",
        "                    array of I/O placeholders.\n",
        "        :param num_loops: The desired number of loops to unroll\n",
        "        :return: A dictionary of the two graphs (deep+shallow).\n",
        "        \"\"\"\n",
        "        # Scoping -- generate the deep/unrolled graph (training)\n",
        "        self.current_depth = self.DEEP\n",
        "        with tf.variable_scope(self.MODEL_NAME, reuse=False):\n",
        "            self.graph_dict[self.DEEP] = self.unroll(func, self.DEEP, num_loops)\n",
        "\n",
        "        # Now, generate the shallow graph (inference)\n",
        "        self.current_depth = self.SHALLOW\n",
        "        with tf.variable_scope(self.MODEL_NAME, reuse=True):\n",
        "            # Shallow is depth 1, but sharing all variables with deep graph above\n",
        "            self.graph_dict[self.SHALLOW] = self.unroll(func, self.SHALLOW, 1)\n",
        "\n",
        "        # pprint(self.graph_dict)\n",
        "        return self.graph_dict\n",
        "\n",
        "    def unroll(self, func, depth_type, num_loops):\n",
        "        \"\"\"\n",
        "        Given the graph-generating function, unroll to the desired depth.\n",
        "        :param func: A function which takes the BPTT object and the depth_type (BPTT.{DEEP,SHALLOW}), returns\n",
        "                    array of I/O placeholders.\n",
        "        :param depth_type: The depth_type (BPTT.{DEEP,SHALLOW})\n",
        "        :param num_loops: The desired number of loops to unroll\n",
        "        :return: A list of the graphs, connected by variables.\n",
        "        \"\"\"\n",
        "        frames = []\n",
        "        for loop in range(num_loops):\n",
        "            # Scoping on top of each depth\n",
        "            # We need 'False' for the first time and 'True' for all others\n",
        "            with tf.variable_scope(self.LOOP_SCOPE, reuse=(loop != 0)):\n",
        "                frames.append(func(self, depth_type))\n",
        "\n",
        "        return frames\n",
        "\n",
        "    def generate_feed_dict(self, depth_type, data_array, num_settable):\n",
        "        \"\"\"\n",
        "        Generate a feed dictionary; takes in an array of the data that will be inserted into the unrolled\n",
        "        placeholders.\n",
        "        :param depth_type: The depth_type (BPTT.{DEEP,SHALLOW})\n",
        "        :param data_array: An array of arrays of data to insert into the unrolled placeholders\n",
        "        :param num_settable: How many elements of the data_array to use.\n",
        "        :return: A dictionary to feed into tf.Session().run()\n",
        "        \"\"\"\n",
        "        frames = self.graph_dict[depth_type]\n",
        "        d = {}\n",
        "\n",
        "        # Recurrent: Auto-defined placeholders / current variables\n",
        "        for variable_name in self.placeholders[depth_type]:\n",
        "            d[self.placeholders[depth_type][variable_name]] = self.state[depth_type][variable_name]\n",
        "\n",
        "        # User-provided data to unroll/insert into the placeholders\n",
        "        for frame_index in range(len(frames)):       # Unroll index\n",
        "            for var_index in range(num_settable):    # Variable index\n",
        "                frame_var = frames[frame_index][var_index]\n",
        "                d[frame_var] = np.reshape(data_array[var_index][frame_index],\n",
        "                                          frame_var.get_shape())\n",
        "        return d\n",
        "\n",
        "    def copy_state_forward(self):\n",
        "        \"\"\"\n",
        "        Copy the working state from the DEEP pipeline to the SHALLOW pipeline\n",
        "        \"\"\"\n",
        "        for key in self.state[self.DEEP]:\n",
        "            self.state[self.SHALLOW][key] = np.copy(self.state[self.DEEP][key])\n",
        "\n",
        "    def generate_output_definitions(self, depth_type):\n",
        "        \"\"\"\n",
        "        Generate the desired output variables to fetch from the graph run\n",
        "        :param depth_type: The depth_type (BPTT.{DEEP,SHALLOW})\n",
        "        :return: An array of variables to add to the fetch list\n",
        "        \"\"\"\n",
        "        d = self.vars[depth_type]\n",
        "        # Define consistent sort order by the variable names\n",
        "        return [d[k] for k in sorted(d.keys())]\n",
        "\n",
        "    def save_output_state(self, depth_type, arr):\n",
        "        \"\"\"\n",
        "        Save the working state for the next run (will be available in generate_feed_dict() in the next loop)\n",
        "        :param depth_type: The depth_type (BPTT.{DEEP,SHALLOW})\n",
        "        :param arr: An array of values (returned by tf.Session.run()) which map to generate_output_definitions()\n",
        "        \"\"\"\n",
        "        d = self.state[depth_type]\n",
        "        sorted_names = sorted(d.keys())\n",
        "        assert len(sorted_names) == len(arr), \\\n",
        "            \"Sent in the wrong number of variables (%s) to update state (%s)\" % (len(arr), len(sorted_names))\n",
        "        for variable_index in range(len(sorted_names)):\n",
        "            variable_name = sorted_names[variable_index]\n",
        "            # Saved for next time.\n",
        "            self.state[depth_type][variable_name] = arr[variable_index]"
      ],
      "metadata": {
        "id": "mlUlB1JfcuV7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase definida en el bloque de código anterior, BPTT, se se va a encargar de guardar los estados de la red recurrente, así como sus placeholders de entrada y salida. También, Creará el grafo \"desdoblado\" de la red recurrente a partir de una función que indiquemos. En el ejemplo se puede ver más a detalle."
      ],
      "metadata": {
        "id": "YrNwoheBdwLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo de uso"
      ],
      "metadata": {
        "id": "MvyTr4DBc_mB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a definir los parámetros que vamos a ocupar para los datos de entrada y salida, el modelo, el optimizador y el entrenamiento."
      ],
      "metadata": {
        "id": "nRpO6GNKnzFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data parameters: simple one-number-at-a-time for now\n",
        "input_dimensions = 1\n",
        "output_dimensions = 1\n",
        "batch_size = 1\n",
        "\n",
        "# Model parameters\n",
        "rnn_width = 3\n",
        "m = 0.0\n",
        "s = 0.5\n",
        "init = tf.random_normal_initializer(m, s)\n",
        "noise_m = 0.0\n",
        "noise_s = 0.03\n",
        "\n",
        "# Optimization parameters\n",
        "learning_rate = 0.05\n",
        "beta1 = 0.95\n",
        "beta2 = .999\n",
        "epsilon = 1e-3\n",
        "momentum = 0.4\n",
        "gradient_clipping = 4.0\n",
        "unroll_depth = 10\n",
        "max_reset_loops = 20\n",
        "\n",
        "# Training parameters\n",
        "num_training_loops = 3000\n",
        "num_inference_loops = 100\n",
        "num_inference_warmup_loops = 1900\n"
      ],
      "metadata": {
        "id": "68eGmjogk183"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rnn_layer(bp, layer_index, raw_x, width):\n",
        "    \"\"\"\n",
        "    Construye una capa RNN de acuerdo a https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
        "    \"\"\"\n",
        "    global init, noise_m, noise_s\n",
        "    # Define variable names\n",
        "    h_name = \"hidden-%s\" % layer_index  \n",
        "    # raw_x is [input_size, 1]\n",
        "    input_size = raw_x.get_shape()[0].value\n",
        "    # Why so serious? Introduce a little anarchy. Upset the established order...\n",
        "    x = raw_x + tf.random_normal(raw_x.get_shape(), noise_m, noise_s)\n",
        "\n",
        "    with tf.variable_scope(\"rnn_layer_%s\" % layer_index):\n",
        "\n",
        "        # Define shapes for all the weights/biases, limited to just this layer (not shared with other layers)\n",
        "        # Sizes are 'input_size' when mapping x and 'width' otherwise\n",
        "        W_ax = tf.get_variable(\"W_ax\", [width, input_size], initializer=init)\n",
        "        W_aa = tf.get_variable(\"W_aa\", [width, width], initializer=init) \n",
        "        b_a =  tf.get_variable(\"b_a\",  [width, 1], initializer=init)\n",
        "        \n",
        "        W_ya = tf.get_variable(\"W_ya\", [width, width], initializer=init)\n",
        "        b_y =  tf.get_variable(\"b_y\",  [width, 1], initializer=init)\n",
        "\n",
        "        # Retrieve the previous roll-depth's data, with starting random data if first roll-depth.\n",
        "        h_past = bp.get_past_variable(h_name, np.float32(np.random.normal(m, s, [width, 1])))\n",
        "\n",
        "        h = bp.name_variable(h_name,  tf.tanh(tf.matmul(W_aa, h_past) + tf.matmul(W_ax, x) + b_a))\n",
        "        o = tf.tanh(tf.matmul(W_ya, h) + b_y)\n",
        "\n",
        "    return [o]"
      ],
      "metadata": {
        "id": "i6gB-Xvjkyyv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_single_rnn_frame(bp, depth_type):\n",
        "    global init, input_dimensions, output_dimensions, batch_size, rnn_width\n",
        "\n",
        "    # I/O DATA\n",
        "    input_placeholder = tf.placeholder(tf.float32, shape=(input_dimensions, batch_size))\n",
        "    output_placeholder = tf.placeholder(tf.float32, shape=(output_dimensions, batch_size))\n",
        "\n",
        "    last_output = input_placeholder\n",
        "    for layer_index in range(1):\n",
        "        [o]= build_rnn_layer(bp, depth_type, layer_index, last_output, rnn_width)\n",
        "        last_output = o\n",
        "\n",
        "    output_result = o\n",
        "\n",
        "    # return array of whatever you want, but I/O placeholders FIRST.\n",
        "    return [input_placeholder, output_placeholder, output_result]"
      ],
      "metadata": {
        "id": "LJfTnUzwlC0k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def palindrome(step):\n",
        "    \"\"\"\n",
        "    Turn sequential integers into a palindromic sequence (so look-ahead mapping is not a function, but requires state)\n",
        "    \"\"\"\n",
        "    return (5.0 - abs(float(step % 10) - 5.0)) / 10.0"
      ],
      "metadata": {
        "id": "GRS6YUAQldTR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bp = None\n",
        "sess = None\n",
        "graphs = None\n",
        "done = False\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Loop until you get out of a local minimum or you hit max reset loops\n",
        "for reset_loop_index in range(max_reset_loops):\n",
        "\n",
        "    # Clean any previous loops\n",
        "    if reset_loop_index > 0:\n",
        "        tf.reset_default_graph()\n",
        "\n",
        "    # Generate unrolled+shallow graphs\n",
        "    bp = BPTT()\n",
        "    graphs = bp.generate_graphs(build_single_rnn_frame, unroll_depth)\n",
        "\n",
        "    # Define loss and clip gradients\n",
        "    error_vec = [[o - p] for [i, p, o] in graphs[bp.DEEP]]\n",
        "    loss = tf.reduce_mean(tf.square(error_vec))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon)\n",
        "    grads = optimizer.compute_gradients(loss)\n",
        "    clipped_grads = [(tf.clip_by_value(grad, -gradient_clipping, gradient_clipping), var) for grad, var in grads]\n",
        "    optimizer.apply_gradients(clipped_grads)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Boilerplate initialization\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init_op)\n",
        "    reset = False\n",
        "\n",
        "    print(\"=== Training the unrolled model (reset loop %s) ===\" % (reset_loop_index))\n",
        "\n",
        "    for step in range(num_training_loops):\n",
        "        # 1.) Generate the dictionary of I/O placeholder data\n",
        "        start_index = step * unroll_depth\n",
        "        in_data = np.array([palindrome(x) for x in range(start_index, start_index + unroll_depth)], dtype=np.float32)\n",
        "        out_data = np.array([palindrome(x+1) for x in range(start_index, start_index + unroll_depth)], dtype=np.float32)\n",
        "\n",
        "        # 2a.) Generate the working state to send in, along with data to insert into unrolled placeholders\n",
        "        frame_dict = bp.generate_feed_dict(bp.DEEP, [in_data, out_data], 2)\n",
        "\n",
        "        # 2b.) Define the output (training/loss) that we'd like to see (optional)\n",
        "        session_out = [train, loss] + [o for [i, p, o] in graphs[bp.DEEP]]   # calculated output\n",
        "\n",
        "        # 3.) Define state variables to pull out as well.\n",
        "        state_vars = bp.generate_output_definitions(bp.DEEP)\n",
        "        session_out.extend(state_vars)\n",
        "\n",
        "        # 4.) Execute the graph\n",
        "        results = sess.run(session_out, feed_dict=frame_dict)\n",
        "\n",
        "        # 5.) Extract the state for next training loop; need to make sure we have right part of result array\n",
        "        bp.save_output_state(bp.DEEP, results[-len(state_vars):])  # for simple RNN\n",
        "\n",
        "        # 6.) Show training progress; reset graph if loss is stagnant.\n",
        "        if (step % 100) == 0:\n",
        "            print(results[0])\n",
        "            print(\"Loss: %s => %s (output: %s)\" % (step, results[1], [str(x) for x in results[2:-len(state_vars)]]))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            if step >= 1000 and (results[1] > 0.01):\n",
        "                print(\"\\nResetting; loss (%s) is stagnating after 1k rounds...\\n\" % (results[1]))\n",
        "                reset = True\n",
        "                break  # To next reset loop\n",
        "\n",
        "    if not reset:\n",
        "        break\n",
        "\n",
        "print(\"=== Evaluating on shallow model ===\")\n",
        "\n",
        "# Copy final deep state from the training loop above to the shallow state.\n",
        "bp.copy_state_forward()\n",
        "[in_ph, out_ph, out_out] = graphs[bp.SHALLOW][0]\n",
        "\n",
        "# Evaluate one step at a time, and burn in first.\n",
        "for step in range(num_inference_loops + num_inference_warmup_loops):\n",
        "    # 1.) Convert step to the palindromic sequence (current and look-ahead-by-one)\n",
        "    in_value = palindrome(step)\n",
        "    expected_out_value = palindrome(step+1)\n",
        "\n",
        "    # 2.) Generate the feed dictionary to send in, both I/O data and recurrent variables\n",
        "    frame_dict = bp.generate_feed_dict(bp.SHALLOW, np.array([[in_value]], np.float32), 1)\n",
        "\n",
        "    # 3.) Define state variables to pull out\n",
        "    session_out = [out_out]\n",
        "    state_vars = bp.generate_output_definitions(bp.SHALLOW)\n",
        "    session_out.extend(state_vars)\n",
        "\n",
        "    # 4.) Execute the graph\n",
        "    results = sess.run(session_out, feed_dict=frame_dict)\n",
        "\n",
        "    # 5.) Extract/save state variables for the next loop\n",
        "    bp.save_output_state(bp.SHALLOW, results[-len(state_vars):])\n",
        "\n",
        "    # 6.) How we doin'?\n",
        "    if step > num_inference_warmup_loops:\n",
        "        print(\"%s: %s => (%s) %s actual vs %s expected (diff: %s)\" %\n",
        "              (step, in_value, np.round(results[0][0][0], 1), results[0][0][0], expected_out_value, expected_out_value - results[0][0][0]))\n",
        "        sys.stdout.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJpgJyXdrGLO",
        "outputId": "2607df74-92ae-4b5a-8ce9-c283a1d6b458"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training the unrolled model (reset loop 0) ===\n",
            "None\n",
            "Loss: 0 => 0.19637738 (output: ['[[-0.72890705]\\n [-0.32281178]\\n [-0.69416434]]', '[[-0.5443011 ]\\n [-0.24014583]\\n [-0.4530898 ]]', '[[-0.33188686]\\n [-0.1299187 ]\\n [-0.13634363]]', '[[-0.19915637]\\n [-0.09391376]\\n [ 0.12934706]]', '[[-0.03247888]\\n [-0.01130863]\\n [ 0.4305343 ]]', '[[0.07504684]\\n [0.02062082]\\n [0.62217516]]', '[[0.15336551]\\n [0.11282057]\\n [0.69603074]]', '[[0.07344776]\\n [0.07460824]\\n [0.6447511 ]]', '[[0.03427105]\\n [0.09117195]\\n [0.5999252 ]]', '[[-0.07985406]\\n [ 0.05318728]\\n [ 0.46335745]]'])\n",
            "None\n",
            "Loss: 100 => 0.006473334 (output: ['[[0.10213082]\\n [0.08570457]\\n [0.079606  ]]', '[[0.19977358]\\n [0.18664253]\\n [0.18466932]]', '[[0.23653129]\\n [0.22355193]\\n [0.22104402]]', '[[0.3333786 ]\\n [0.31994775]\\n [0.32064646]]', '[[0.36226377]\\n [0.34880826]\\n [0.3486868 ]]', '[[0.43517402]\\n [0.42330736]\\n [0.42542332]]', '[[0.33010107]\\n [0.31216863]\\n [0.31081223]]', '[[0.2623094 ]\\n [0.24255002]\\n [0.23945168]]', '[[0.25958052]\\n [0.24105042]\\n [0.23846541]]', '[[0.0960828 ]\\n [0.07764896]\\n [0.0674023 ]]'])\n",
            "None\n",
            "Loss: 200 => 0.0026201308 (output: ['[[0.01839753]\\n [0.02089317]\\n [0.02105543]]', '[[0.1773071 ]\\n [0.17402214]\\n [0.18503201]]', '[[0.31432173]\\n [0.31877095]\\n [0.32587674]]', '[[0.40300974]\\n [0.41048685]\\n [0.4127453 ]]', '[[0.44202724]\\n [0.44723585]\\n [0.44640568]]', '[[0.44520867]\\n [0.445643  ]\\n [0.44404033]]', '[[0.3286824 ]\\n [0.32536408]\\n [0.32302845]]', '[[0.23721819]\\n [0.23514661]\\n [0.23115145]]', '[[0.1283194 ]\\n [0.12951034]\\n [0.12399314]]', '[[0.1062382 ]\\n [0.10632106]\\n [0.10498601]]'])\n",
            "None\n",
            "Loss: 300 => 0.0031407168 (output: ['[[0.0389024 ]\\n [0.03802798]\\n [0.03988368]]', '[[0.19081585]\\n [0.18305413]\\n [0.1888718 ]]', '[[0.33282828]\\n [0.32914868]\\n [0.33174193]]', '[[0.48180345]\\n [0.48789182]\\n [0.4838595 ]]', '[[0.5162856 ]\\n [0.52124363]\\n [0.5185724 ]]', '[[0.4796076 ]\\n [0.47796676]\\n [0.48041573]]', '[[0.30521736]\\n [0.29808977]\\n [0.30497667]]', '[[0.15962112]\\n [0.15715902]\\n [0.161044  ]]', '[[0.13223593]\\n [0.12973528]\\n [0.13343096]]', '[[0.10038894]\\n [0.09867035]\\n [0.10157057]]'])\n",
            "None\n",
            "Loss: 400 => 0.0024939158 (output: ['[[0.05875183]\\n [0.06315909]\\n [0.05984795]]', '[[0.23385955]\\n [0.23132102]\\n [0.23297924]]', '[[0.36834294]\\n [0.36809435]\\n [0.36846414]]', '[[0.4522676 ]\\n [0.45615405]\\n [0.4537216 ]]', '[[0.45523342]\\n [0.45851877]\\n [0.45614028]]', '[[0.40786156]\\n [0.40965047]\\n [0.40794924]]', '[[0.2935355 ]\\n [0.29275936]\\n [0.29243058]]', '[[0.21029276]\\n [0.211157  ]\\n [0.20967956]]', '[[0.14937969]\\n [0.15138899]\\n [0.14918403]]', '[[0.09921182]\\n [0.10272307]\\n [0.09964927]]'])\n",
            "None\n",
            "Loss: 500 => 0.0036480664 (output: ['[[-0.01406858]\\n [-0.00856915]\\n [-0.01228766]]', '[[0.11035895]\\n [0.10907929]\\n [0.10883743]]', '[[0.26727867]\\n [0.26375285]\\n [0.26440027]]', '[[0.44192535]\\n [0.44914806]\\n [0.44330248]]', '[[0.508758  ]\\n [0.51656973]\\n [0.5108823 ]]', '[[0.48323172]\\n [0.49054423]\\n [0.48549688]]', '[[0.32232133]\\n [0.31361163]\\n [0.31863633]]', '[[0.1433901 ]\\n [0.14055648]\\n [0.14228944]]', '[[0.06630839]\\n [0.06531542]\\n [0.06593111]]', '[[-0.01022579]\\n [-0.00615205]\\n [-0.00869715]]'])\n",
            "None\n",
            "Loss: 600 => 0.001121702 (output: ['[[0.07411207]\\n [0.08847359]\\n [0.07740401]]', '[[0.22528438]\\n [0.22971147]\\n [0.22461429]]', '[[0.33748925]\\n [0.3402922 ]\\n [0.33630857]]', '[[0.43173766]\\n [0.44197443]\\n [0.43385765]]', '[[0.4623429 ]\\n [0.47329456]\\n [0.4650292 ]]', '[[0.41832468]\\n [0.42995802]\\n [0.42127672]]', '[[0.3137468 ]\\n [0.3135413 ]\\n [0.31169736]]', '[[0.17927828]\\n [0.18282261]\\n [0.17852804]]', '[[0.09833173]\\n [0.10615923]\\n [0.09918561]]', '[[0.06608314]\\n [0.07813387]\\n [0.06854723]]'])\n",
            "None\n",
            "Loss: 700 => 0.0006125435 (output: ['[[0.07583703]\\n [0.07760023]\\n [0.07557885]]', '[[0.21406898]\\n [0.20643014]\\n [0.2103119 ]]', '[[0.342264  ]\\n [0.33433503]\\n [0.3385706 ]]', '[[0.44789836]\\n [0.4532893 ]\\n [0.44949958]]', '[[0.4816894 ]\\n [0.48842186]\\n [0.48374054]]', '[[0.42433828]\\n [0.41567472]\\n [0.42011407]]', '[[0.2816019]\\n [0.2817328]\\n [0.2805904]]', '[[0.1825252 ]\\n [0.18175651]\\n [0.18112552]]', '[[0.10104545]\\n [0.0924999 ]\\n [0.09663533]]', '[[0.00694952]\\n [0.0109362 ]\\n [0.00744411]]'])\n",
            "None\n",
            "Loss: 800 => 0.001633164 (output: ['[[0.04396529]\\n [0.0459982 ]\\n [0.04398957]]', '[[0.17051013]\\n [0.1638784 ]\\n [0.16734287]]', '[[0.32280865]\\n [0.31442437]\\n [0.31876493]]', '[[0.46436238]\\n [0.46622783]\\n [0.46310577]]', '[[0.4917557 ]\\n [0.49211243]\\n [0.48919663]]', '[[0.4392658 ]\\n [0.43320036]\\n [0.433356  ]]', '[[0.34255615]\\n [0.33639473]\\n [0.33604053]]', '[[0.18390073]\\n [0.17957345]\\n [0.17918327]]', '[[0.04570801]\\n [0.04210594]\\n [0.04271434]]', '[[-0.03919069]\\n [-0.03072364]\\n [-0.03688703]]'])\n",
            "None\n",
            "Loss: 900 => 0.00044270154 (output: ['[[0.09246694]\\n [0.09445099]\\n [0.09148221]]', '[[0.2007487 ]\\n [0.19826214]\\n [0.19797729]]', '[[0.31023225]\\n [0.30580857]\\n [0.30683777]]', '[[0.4238701 ]\\n [0.42365104]\\n [0.42267582]]', '[[0.47578222]\\n [0.48047894]\\n [0.47703096]]', '[[0.38572732]\\n [0.38530308]\\n [0.38461167]]', '[[0.26799297]\\n [0.2676305 ]\\n [0.26667106]]', '[[0.16894414]\\n [0.16672675]\\n [0.16646674]]', '[[0.07598213]\\n [0.07318676]\\n [0.07294131]]', '[[0.01360379]\\n [0.0186644 ]\\n [0.01387701]]'])\n",
            "None\n",
            "Loss: 1000 => 0.0016643757 (output: ['[[0.09408372]\\n [0.09462712]\\n [0.09688926]]', '[[0.21717265]\\n [0.21406192]\\n [0.21805434]]', '[[0.3708234 ]\\n [0.37069884]\\n [0.37151733]]', '[[0.45827335]\\n [0.4604601 ]\\n [0.45909584]]', '[[0.4407174 ]\\n [0.44003868]\\n [0.44086406]]', '[[0.39081702]\\n [0.38855228]\\n [0.39087862]]', '[[0.28707832]\\n [0.28649977]\\n [0.28833663]]', '[[0.17885745]\\n [0.17612414]\\n [0.18021812]]', '[[0.04009685]\\n [0.04052094]\\n [0.04326778]]', '[[-0.01155975]\\n [-0.00658359]\\n [-0.00677209]]'])\n",
            "None\n",
            "Loss: 1100 => 0.0017072178 (output: ['[[0.1276281 ]\\n [0.12681285]\\n [0.13205197]]', '[[0.26414892]\\n [0.2667638 ]\\n [0.2707077 ]]', '[[0.34206456]\\n [0.34416455]\\n [0.3489076 ]]', '[[0.41641194]\\n [0.41758695]\\n [0.42328367]]', '[[0.42231885]\\n [0.42410812]\\n [0.42946008]]', '[[0.36118114]\\n [0.36428124]\\n [0.3685845 ]]', '[[0.29616916]\\n [0.29863644]\\n [0.3030578 ]]', '[[0.18693006]\\n [0.18855189]\\n [0.19284244]]', '[[0.0759642 ]\\n [0.07856506]\\n [0.08130025]]', '[[0.0413928 ]\\n [0.04230711]\\n [0.04575624]]'])\n",
            "None\n",
            "Loss: 1200 => 0.0014346871 (output: ['[[0.11435383]\\n [0.11489363]\\n [0.11698103]]', '[[0.21849152]\\n [0.2199737 ]\\n [0.22125462]]', '[[0.31145337]\\n [0.31295815]\\n [0.31409374]]', '[[0.41409475]\\n [0.41453493]\\n [0.4162925 ]]', '[[0.44234806]\\n [0.44217739]\\n [0.44431832]]', '[[0.40126052]\\n [0.4020031 ]\\n [0.403486  ]]', '[[0.3315683]\\n [0.3324066]\\n [0.3339207]]', '[[0.2520359 ]\\n [0.25273585]\\n [0.25445944]]', '[[0.14966393]\\n [0.15071616]\\n [0.1522642 ]]', '[[0.0585542 ]\\n [0.05973087]\\n [0.06127577]]'])\n",
            "None\n",
            "Loss: 1300 => 0.00044448767 (output: ['[[0.09606299]\\n [0.09506618]\\n [0.09665774]]', '[[0.22715005]\\n [0.22739364]\\n [0.22760436]]', '[[0.32131168]\\n [0.32267007]\\n [0.32166636]]', '[[0.41661677]\\n [0.4165283 ]\\n [0.4166587 ]]', '[[0.48090062]\\n [0.47929466]\\n [0.4806827 ]]', '[[0.39475843]\\n [0.39518955]\\n [0.39481512]]', '[[0.31146365]\\n [0.31237522]\\n [0.31169686]]', '[[0.22242111]\\n [0.22270694]\\n [0.22276787]]', '[[0.12522173]\\n [0.12547502]\\n [0.12574938]]', '[[0.03410359]\\n [0.03519904]\\n [0.03489025]]'])\n",
            "None\n",
            "Loss: 1400 => 0.00042645397 (output: ['[[0.06455025]\\n [0.06521385]\\n [0.06912633]]', '[[0.18512079]\\n [0.18534462]\\n [0.18990055]]', '[[0.28432527]\\n [0.2857955 ]\\n [0.28886473]]', '[[0.38091403]\\n [0.3821278 ]\\n [0.38520095]]', '[[0.49427828]\\n [0.4940115 ]\\n [0.49819425]]', '[[0.41482693]\\n [0.4153716 ]\\n [0.4187718 ]]', '[[0.32191288]\\n [0.32322457]\\n [0.3259044 ]]', '[[0.23808524]\\n [0.23888268]\\n [0.24221452]]', '[[0.09615582]\\n [0.09686941]\\n [0.10027604]]', '[[0.00021626]\\n [0.00176041]\\n [0.00426281]]'])\n",
            "None\n",
            "Loss: 1500 => 0.0004686476 (output: ['[[0.11455747]\\n [0.11471049]\\n [0.11597849]]', '[[0.21592772]\\n [0.21599425]\\n [0.2174233 ]]', '[[0.30925715]\\n [0.3097399 ]\\n [0.31065264]]', '[[0.40162194]\\n [0.40207365]\\n [0.40293494]]', '[[0.4913181 ]\\n [0.49118584]\\n [0.49258384]]', '[[0.40362477]\\n [0.40385637]\\n [0.40485615]]', '[[0.30659682]\\n [0.30709943]\\n [0.30780596]]', '[[0.25808015]\\n [0.2584463 ]\\n [0.2593232 ]]', '[[0.12201518]\\n [0.12219942]\\n [0.12327402]]', '[[-0.00241954]\\n [-0.00186793]\\n [-0.00126597]]'])\n",
            "None\n",
            "Loss: 1600 => 9.8752214e-05 (output: ['[[0.09244844]\\n [0.09268037]\\n [0.09334008]]', '[[0.20875682]\\n [0.20890442]\\n [0.20968813]]', '[[0.3070434 ]\\n [0.30717972]\\n [0.30755258]]', '[[0.40939426]\\n [0.40943775]\\n [0.40948775]]', '[[0.49684975]\\n [0.49659306]\\n [0.49688143]]', '[[0.41459534]\\n [0.41440797]\\n [0.4145613 ]]', '[[0.29829702]\\n [0.29825333]\\n [0.29829994]]', '[[0.22187637]\\n [0.22185828]\\n [0.22215672]]', '[[0.10135141]\\n [0.10141814]\\n [0.10196583]]', '[[-0.00151272]\\n [-0.00125971]\\n [-0.00080016]]'])\n",
            "None\n",
            "Loss: 1700 => 0.00056516106 (output: ['[[0.10590659]\\n [0.10559569]\\n [0.10520171]]', '[[0.21322742]\\n [0.21297687]\\n [0.21257718]]', '[[0.32811216]\\n [0.3279097 ]\\n [0.32730892]]', '[[0.43374154]\\n [0.43346938]\\n [0.43273425]]', '[[0.5047691 ]\\n [0.504247  ]\\n [0.50358427]]', '[[0.3712409 ]\\n [0.37072462]\\n [0.36995843]]', '[[0.26885313]\\n [0.26834768]\\n [0.2676293 ]]', '[[0.19094543]\\n [0.19043489]\\n [0.18983918]]', '[[0.06061101]\\n [0.06013439]\\n [0.05971751]]', '[[0.00255103]\\n [0.00219312]\\n [0.00178595]]'])\n",
            "None\n",
            "Loss: 1800 => 0.00064339384 (output: ['[[0.06402361]\\n [0.06401539]\\n [0.06367305]]', '[[0.19537087]\\n [0.195427  ]\\n [0.19508599]]', '[[0.30671206]\\n [0.30671114]\\n [0.3062373 ]]', '[[0.44400406]\\n [0.44378605]\\n [0.443018  ]]', '[[0.52358395]\\n [0.52309805]\\n [0.5220683 ]]', '[[0.41723666]\\n [0.41671816]\\n [0.41564968]]', '[[0.32338494]\\n [0.32290447]\\n [0.3218978 ]]', '[[0.21350136]\\n [0.21312991]\\n [0.21230853]]', '[[0.06125391]\\n [0.06103888]\\n [0.06048476]]', '[[-0.01215609]\\n [-0.01224638]\\n [-0.01263062]]'])\n",
            "None\n",
            "Loss: 1900 => 0.00087957655 (output: ['[[0.10451953]\\n [0.10466741]\\n [0.10459543]]', '[[0.20303488]\\n [0.20307912]\\n [0.20293745]]', '[[0.26528153]\\n [0.26533124]\\n [0.2652328 ]]', '[[0.3587691 ]\\n [0.35889748]\\n [0.3589577 ]]', '[[0.4552321 ]\\n [0.45543078]\\n [0.45565796]]', '[[0.41504252]\\n [0.41529578]\\n [0.41557008]]', '[[0.27764702]\\n [0.27794567]\\n [0.27818528]]', '[[0.1779496 ]\\n [0.17821875]\\n [0.17835787]]', '[[0.06523204]\\n [0.0654993 ]\\n [0.06556167]]', '[[0.03844704]\\n [0.03868368]\\n [0.03868573]]'])\n",
            "None\n",
            "Loss: 2000 => 0.00027363453 (output: ['[[0.09270113]\\n [0.09234034]\\n [0.09245616]]', '[[0.20754144]\\n [0.20723134]\\n [0.20723252]]', '[[0.30997926]\\n [0.30980837]\\n [0.30973697]]', '[[0.4231006 ]\\n [0.42302063]\\n [0.4228848 ]]', '[[0.512982  ]\\n [0.5127599 ]\\n [0.51257473]]', '[[0.42899024]\\n [0.42874944]\\n [0.4286271 ]]', '[[0.3142083 ]\\n [0.31392783]\\n [0.31389272]]', '[[0.20756729]\\n [0.20718248]\\n [0.20722404]]', '[[0.07309064]\\n [0.07264545]\\n [0.07279536]]', '[[-0.00437696]\\n [-0.0047602 ]\\n [-0.00454552]]'])\n",
            "None\n",
            "Loss: 2100 => 0.0001156545 (output: ['[[0.11313662]\\n [0.1134409 ]\\n [0.11426437]]', '[[0.19767259]\\n [0.19791801]\\n [0.19873099]]', '[[0.2869065]\\n [0.2870643]\\n [0.2877882]]', '[[0.3962587 ]\\n [0.39633593]\\n [0.39692733]]', '[[0.5007151 ]\\n [0.50077844]\\n [0.5012515 ]]', '[[0.4142541 ]\\n [0.4143626 ]\\n [0.41488707]]', '[[0.3033754 ]\\n [0.30355087]\\n [0.3041576 ]]', '[[0.20834829]\\n [0.20860295]\\n [0.20929864]]', '[[0.08189347]\\n [0.08223374]\\n [0.08301333]]', '[[0.01289165]\\n [0.01325243]\\n [0.01406146]]'])\n",
            "None\n",
            "Loss: 2200 => 0.00079029775 (output: ['[[0.10435783]\\n [0.10443835]\\n [0.10501157]]', '[[0.24241121]\\n [0.24250233]\\n [0.24302152]]', '[[0.33923614]\\n [0.3394302 ]\\n [0.34000257]]', '[[0.4322923 ]\\n [0.43256402]\\n [0.43317315]]', '[[0.49313894]\\n [0.4933985 ]\\n [0.4939801 ]]', '[[0.35136807]\\n [0.3516576 ]\\n [0.35235444]]', '[[0.27675775]\\n [0.27702925]\\n [0.27774584]]', '[[0.19642493]\\n [0.19661993]\\n [0.1972954 ]]', '[[0.0766523 ]\\n [0.07676332]\\n [0.07739559]]', '[[-0.00085714]\\n [-0.00076628]\\n [-0.00014129]]'])\n",
            "None\n",
            "Loss: 2300 => 0.00024498516 (output: ['[[0.0864417 ]\\n [0.08660793]\\n [0.08675319]]', '[[0.20604686]\\n [0.20619152]\\n [0.20625539]]', '[[0.30433923]\\n [0.30441654]\\n [0.3045387 ]]', '[[0.42794085]\\n [0.42788   ]\\n [0.42804807]]', '[[0.51822734]\\n [0.5180457 ]\\n [0.51807547]]', '[[0.40441364]\\n [0.40425766]\\n [0.4043963 ]]', '[[0.31397256]\\n [0.3138685 ]\\n [0.31405047]]', '[[0.22245988]\\n [0.2224373 ]\\n [0.22258186]]', '[[0.10275717]\\n [0.10283452]\\n [0.10297613]]', '[[0.01919705]\\n [0.01934498]\\n [0.01953632]]'])\n",
            "None\n",
            "Loss: 2400 => 0.00029321157 (output: ['[[0.06825328]\\n [0.06760173]\\n [0.06612115]]', '[[0.18715464]\\n [0.18649644]\\n [0.18500428]]', '[[0.2892499 ]\\n [0.2886789 ]\\n [0.28740704]]', '[[0.4093569 ]\\n [0.40881234]\\n [0.40768126]]', '[[0.50203896]\\n [0.50131196]\\n [0.49990404]]', '[[0.41212404]\\n [0.41138345]\\n [0.40993577]]', '[[0.30868664]\\n [0.3079444 ]\\n [0.3064566 ]]', '[[0.20414375]\\n [0.20336449]\\n [0.20174898]]', '[[0.0700679 ]\\n [0.06931797]\\n [0.06769913]]', '[[-0.01656381]\\n [-0.01720834]\\n [-0.01865963]]'])\n",
            "None\n",
            "Loss: 2500 => 0.0022417726 (output: ['[[0.15102711]\\n [0.15108535]\\n [0.15147361]]', '[[0.2677185 ]\\n [0.2677615 ]\\n [0.26795557]]', '[[0.3520768 ]\\n [0.35220525]\\n [0.3523898 ]]', '[[0.43305495]\\n [0.43325   ]\\n [0.4334974 ]]', '[[0.48148146]\\n [0.48165715]\\n [0.48195165]]', '[[0.31651616]\\n [0.31673735]\\n [0.3172541 ]]', '[[0.25336078]\\n [0.2535663 ]\\n [0.25412774]]', '[[0.18464847]\\n [0.18478817]\\n [0.1853391 ]]', '[[0.08517039]\\n [0.08525681]\\n [0.08583177]]', '[[0.03883312]\\n [0.03891846]\\n [0.03949619]]'])\n",
            "None\n",
            "Loss: 2600 => 0.0014423291 (output: ['[[0.12185049]\\n [0.12174098]\\n [0.1218172 ]]', '[[0.236514  ]\\n [0.23644766]\\n [0.23647198]]', '[[0.33335716]\\n [0.33346683]\\n [0.33355567]]', '[[0.44543317]\\n [0.4456438 ]\\n [0.44569382]]', '[[0.52136105]\\n [0.52145654]\\n [0.52130646]]', '[[0.36237714]\\n [0.3624986 ]\\n [0.36248842]]', '[[0.26794758]\\n [0.2679979 ]\\n [0.26801643]]', '[[0.1557204 ]\\n [0.1556368 ]\\n [0.15565644]]', '[[0.03253042]\\n [0.03238875]\\n [0.03248775]]', '[[-0.00152613]\\n [-0.00164312]\\n [-0.00147328]]'])\n",
            "None\n",
            "Loss: 2700 => 0.0007358874 (output: ['[[0.07797594]\\n [0.0777595 ]\\n [0.07662857]]', '[[0.21041845]\\n [0.21013549]\\n [0.20892116]]', '[[0.3252434 ]\\n [0.3248946 ]\\n [0.32405025]]', '[[0.44617364]\\n [0.4456459 ]\\n [0.44489956]]', '[[0.50253606]\\n [0.5018665 ]\\n [0.5009463 ]]', '[[0.35841623]\\n [0.35781687]\\n [0.35699633]]', '[[0.31785375]\\n [0.31728417]\\n [0.31645295]]', '[[0.2421455 ]\\n [0.24163634]\\n [0.24063057]]', '[[0.11038638]\\n [0.11001024]\\n [0.10886503]]', '[[-0.01227006]\\n [-0.0124797 ]\\n [-0.01350352]]'])\n",
            "None\n",
            "Loss: 2800 => 0.0004471834 (output: ['[[0.06605937]\\n [0.06618997]\\n [0.06614757]]', '[[0.16886395]\\n [0.16897318]\\n [0.16914082]]', '[[0.2719545 ]\\n [0.27202353]\\n [0.2723225 ]]', '[[0.37970963]\\n [0.37965244]\\n [0.37969354]]', '[[0.4759079]\\n [0.4757214]\\n [0.4754271]]', '[[0.38968593]\\n [0.38954055]\\n [0.38922197]]', '[[0.31012693]\\n [0.31002167]\\n [0.30966598]]', '[[0.21580093]\\n [0.21573947]\\n [0.21530245]]', '[[0.08901507]\\n [0.08903351]\\n [0.08857086]]', '[[-0.00192868]\\n [-0.00181299]\\n [-0.00211322]]'])\n",
            "None\n",
            "Loss: 2900 => 0.00037365052 (output: ['[[0.06686293]\\n [0.06686852]\\n [0.0668351 ]]', '[[0.19585334]\\n [0.19586495]\\n [0.19583392]]', '[[0.29786676]\\n [0.29792744]\\n [0.29792544]]', '[[0.3965403 ]\\n [0.39660925]\\n [0.3966329 ]]', '[[0.49098077]\\n [0.49103582]\\n [0.49107885]]', '[[0.3908506 ]\\n [0.39091   ]\\n [0.39094943]]', '[[0.29666084]\\n [0.2967089 ]\\n [0.2967352 ]]', '[[0.18225177]\\n [0.18227282]\\n [0.18227561]]', '[[0.06395226]\\n [0.06396324]\\n [0.06394805]]', '[[0.028585  ]\\n [0.02859382]\\n [0.02856766]]'])\n",
            "=== Evaluating on shallow model ===\n",
            "1901: 0.1 => (0.2) 0.19533627 actual vs 0.2 expected (diff: 0.004663732647895824)\n",
            "1902: 0.2 => (0.3) 0.28794685 actual vs 0.3 expected (diff: 0.012053149938583363)\n",
            "1903: 0.3 => (0.4) 0.3920132 actual vs 0.4 expected (diff: 0.007986807823181175)\n",
            "1904: 0.4 => (0.5) 0.4758763 actual vs 0.5 expected (diff: 0.024123698472976685)\n",
            "1905: 0.5 => (0.4) 0.35370132 actual vs 0.4 expected (diff: 0.04629867672920229)\n",
            "1906: 0.4 => (0.3) 0.28342327 actual vs 0.3 expected (diff: 0.016576725244522084)\n",
            "1907: 0.3 => (0.2) 0.21195576 actual vs 0.2 expected (diff: -0.011955755949020375)\n",
            "1908: 0.2 => (0.1) 0.11011958 actual vs 0.1 expected (diff: -0.010119581222534174)\n",
            "1909: 0.1 => (0.0) 0.023169942 actual vs 0.0 expected (diff: -0.02316994220018387)\n",
            "1910: 0.0 => (0.1) 0.058361053 actual vs 0.1 expected (diff: 0.04163894653320313)\n",
            "1911: 0.1 => (0.2) 0.20316131 actual vs 0.2 expected (diff: -0.0031613141298293956)\n",
            "1912: 0.2 => (0.3) 0.29617852 actual vs 0.3 expected (diff: 0.0038214802742004284)\n",
            "1913: 0.3 => (0.4) 0.4109387 actual vs 0.4 expected (diff: -0.010938709974288918)\n",
            "1914: 0.4 => (0.5) 0.46244544 actual vs 0.5 expected (diff: 0.03755456209182739)\n",
            "1915: 0.5 => (0.3) 0.32758036 actual vs 0.4 expected (diff: 0.07241963744163515)\n",
            "1916: 0.4 => (0.3) 0.27327073 actual vs 0.3 expected (diff: 0.026729273796081532)\n",
            "1917: 0.3 => (0.2) 0.19746909 actual vs 0.2 expected (diff: 0.002530914545059215)\n",
            "1918: 0.2 => (0.1) 0.100482404 actual vs 0.1 expected (diff: -0.00048240423202514093)\n",
            "1919: 0.1 => (0.0) 0.026751196 actual vs 0.0 expected (diff: -0.026751196011900902)\n",
            "1920: 0.0 => (0.1) 0.07334383 actual vs 0.1 expected (diff: 0.02665617167949677)\n",
            "1921: 0.1 => (0.2) 0.19211973 actual vs 0.2 expected (diff: 0.007880267500877391)\n",
            "1922: 0.2 => (0.3) 0.28087744 actual vs 0.3 expected (diff: 0.019122558832168568)\n",
            "1923: 0.3 => (0.4) 0.37841913 actual vs 0.4 expected (diff: 0.021580868959426902)\n",
            "1924: 0.4 => (0.5) 0.47258046 actual vs 0.5 expected (diff: 0.02741953730583191)\n",
            "1925: 0.5 => (0.4) 0.37725982 actual vs 0.4 expected (diff: 0.02274017930030825)\n",
            "1926: 0.4 => (0.3) 0.2879168 actual vs 0.3 expected (diff: 0.01208319067955016)\n",
            "1927: 0.3 => (0.2) 0.22376937 actual vs 0.2 expected (diff: -0.02376936674118041)\n",
            "1928: 0.2 => (0.1) 0.115987256 actual vs 0.1 expected (diff: -0.015987256169319147)\n",
            "1929: 0.1 => (0.0) 0.028403047 actual vs 0.0 expected (diff: -0.02840304747223854)\n",
            "1930: 0.0 => (0.1) 0.0589673 actual vs 0.1 expected (diff: 0.041032700240612036)\n",
            "1931: 0.1 => (0.2) 0.19547993 actual vs 0.2 expected (diff: 0.004520070552825939)\n",
            "1932: 0.2 => (0.3) 0.26403755 actual vs 0.3 expected (diff: 0.03596245050430297)\n",
            "1933: 0.3 => (0.3) 0.3453787 actual vs 0.4 expected (diff: 0.05462130308151247)\n",
            "1934: 0.4 => (0.5) 0.45512474 actual vs 0.5 expected (diff: 0.044875264167785645)\n",
            "1935: 0.5 => (0.4) 0.40802202 actual vs 0.4 expected (diff: -0.008022016286849953)\n",
            "1936: 0.4 => (0.3) 0.28149965 actual vs 0.3 expected (diff: 0.018500345945358265)\n",
            "1937: 0.3 => (0.2) 0.19371405 actual vs 0.2 expected (diff: 0.006285947561264049)\n",
            "1938: 0.2 => (0.1) 0.09457408 actual vs 0.1 expected (diff: 0.005425921082496649)\n",
            "1939: 0.1 => (0.0) 0.03609809 actual vs 0.0 expected (diff: -0.036098089069128036)\n",
            "1940: 0.0 => (0.1) 0.07343528 actual vs 0.1 expected (diff: 0.026564723253250128)\n",
            "1941: 0.1 => (0.2) 0.19840072 actual vs 0.2 expected (diff: 0.0015992790460586659)\n",
            "1942: 0.2 => (0.3) 0.2826823 actual vs 0.3 expected (diff: 0.017317700386047352)\n",
            "1943: 0.3 => (0.4) 0.38340846 actual vs 0.4 expected (diff: 0.01659154295921328)\n",
            "1944: 0.4 => (0.5) 0.456278 actual vs 0.5 expected (diff: 0.043722003698349)\n",
            "1945: 0.5 => (0.4) 0.35167882 actual vs 0.4 expected (diff: 0.04832118153572085)\n",
            "1946: 0.4 => (0.3) 0.2839568 actual vs 0.3 expected (diff: 0.016043204069137562)\n",
            "1947: 0.3 => (0.2) 0.2330576 actual vs 0.2 expected (diff: -0.03305760324001311)\n",
            "1948: 0.2 => (0.1) 0.13593753 actual vs 0.1 expected (diff: -0.03593752682209014)\n",
            "1949: 0.1 => (0.0) 0.029596752 actual vs 0.0 expected (diff: -0.029596751555800438)\n",
            "1950: 0.0 => (0.0) 0.043319184 actual vs 0.1 expected (diff: 0.05668081566691399)\n",
            "1951: 0.1 => (0.2) 0.18300644 actual vs 0.2 expected (diff: 0.016993564367294323)\n",
            "1952: 0.2 => (0.3) 0.27266762 actual vs 0.3 expected (diff: 0.027332383394241322)\n",
            "1953: 0.3 => (0.4) 0.36965933 actual vs 0.4 expected (diff: 0.030340665578842185)\n",
            "1954: 0.4 => (0.5) 0.45662594 actual vs 0.5 expected (diff: 0.043374061584472656)\n",
            "1955: 0.5 => (0.4) 0.35628095 actual vs 0.4 expected (diff: 0.04371904730796816)\n",
            "1956: 0.4 => (0.3) 0.26800773 actual vs 0.3 expected (diff: 0.03199227452278136)\n",
            "1957: 0.3 => (0.2) 0.18443336 actual vs 0.2 expected (diff: 0.015566644072532665)\n",
            "1958: 0.2 => (0.1) 0.08946086 actual vs 0.1 expected (diff: 0.010539142787456518)\n",
            "1959: 0.1 => (0.0) 0.031063091 actual vs 0.0 expected (diff: -0.03106309100985527)\n",
            "1960: 0.0 => (0.1) 0.08223104 actual vs 0.1 expected (diff: 0.017768962681293493)\n",
            "1961: 0.1 => (0.2) 0.20306404 actual vs 0.2 expected (diff: -0.003064039349555958)\n",
            "1962: 0.2 => (0.3) 0.2955727 actual vs 0.3 expected (diff: 0.004427301883697499)\n",
            "1963: 0.3 => (0.4) 0.38914138 actual vs 0.4 expected (diff: 0.01085861921310427)\n",
            "1964: 0.4 => (0.5) 0.46524927 actual vs 0.5 expected (diff: 0.03475072979927063)\n",
            "1965: 0.5 => (0.3) 0.34560356 actual vs 0.4 expected (diff: 0.05439644455909731)\n",
            "1966: 0.4 => (0.3) 0.26999405 actual vs 0.3 expected (diff: 0.03000594973564147)\n",
            "1967: 0.3 => (0.2) 0.18538295 actual vs 0.2 expected (diff: 0.01461705267429353)\n",
            "1968: 0.2 => (0.1) 0.10233326 actual vs 0.1 expected (diff: -0.0023332625627517645)\n",
            "1969: 0.1 => (0.0) 0.033678707 actual vs 0.0 expected (diff: -0.03367870673537254)\n",
            "1970: 0.0 => (0.1) 0.069063924 actual vs 0.1 expected (diff: 0.030936075747013098)\n",
            "1971: 0.1 => (0.2) 0.1920639 actual vs 0.2 expected (diff: 0.007936102151870739)\n",
            "1972: 0.2 => (0.3) 0.28071338 actual vs 0.3 expected (diff: 0.01928662061691283)\n",
            "1973: 0.3 => (0.4) 0.3849788 actual vs 0.4 expected (diff: 0.015021198987960838)\n",
            "1974: 0.4 => (0.5) 0.470787 actual vs 0.5 expected (diff: 0.029213011264801025)\n",
            "1975: 0.5 => (0.4) 0.35522604 actual vs 0.4 expected (diff: 0.04477396011352541)\n",
            "1976: 0.4 => (0.3) 0.2715249 actual vs 0.3 expected (diff: 0.028475093841552723)\n",
            "1977: 0.3 => (0.2) 0.19743082 actual vs 0.2 expected (diff: 0.002569180727005016)\n",
            "1978: 0.2 => (0.1) 0.094265886 actual vs 0.1 expected (diff: 0.005734114348888403)\n",
            "1979: 0.1 => (0.0) 0.03040068 actual vs 0.0 expected (diff: -0.030400680378079414)\n",
            "1980: 0.0 => (0.1) 0.06827679 actual vs 0.1 expected (diff: 0.03172320723533631)\n",
            "1981: 0.1 => (0.2) 0.1962451 actual vs 0.2 expected (diff: 0.0037548959255218617)\n",
            "1982: 0.2 => (0.3) 0.2903591 actual vs 0.3 expected (diff: 0.009640890359878529)\n",
            "1983: 0.3 => (0.4) 0.3990519 actual vs 0.4 expected (diff: 0.0009480953216552956)\n",
            "1984: 0.4 => (0.5) 0.4766336 actual vs 0.5 expected (diff: 0.02336639165878296)\n",
            "1985: 0.5 => (0.3) 0.34277222 actual vs 0.4 expected (diff: 0.05722778439521792)\n",
            "1986: 0.4 => (0.3) 0.27056354 actual vs 0.3 expected (diff: 0.029436457157135)\n",
            "1987: 0.3 => (0.2) 0.20125806 actual vs 0.2 expected (diff: -0.0012580633163452037)\n",
            "1988: 0.2 => (0.1) 0.10955941 actual vs 0.1 expected (diff: -0.009559409320354456)\n",
            "1989: 0.1 => (0.0) 0.032555453 actual vs 0.0 expected (diff: -0.03255545347929001)\n",
            "1990: 0.0 => (0.1) 0.06819373 actual vs 0.1 expected (diff: 0.031806266307830816)\n",
            "1991: 0.1 => (0.2) 0.19325136 actual vs 0.2 expected (diff: 0.006748643517494213)\n",
            "1992: 0.2 => (0.3) 0.28001913 actual vs 0.3 expected (diff: 0.019980865716934193)\n",
            "1993: 0.3 => (0.4) 0.38310122 actual vs 0.4 expected (diff: 0.01689877510070803)\n",
            "1994: 0.4 => (0.4) 0.44211856 actual vs 0.5 expected (diff: 0.057881444692611694)\n",
            "1995: 0.5 => (0.3) 0.3171024 actual vs 0.4 expected (diff: 0.08289759755134585)\n",
            "1996: 0.4 => (0.3) 0.2611543 actual vs 0.3 expected (diff: 0.03884570598602294)\n",
            "1997: 0.3 => (0.2) 0.17606936 actual vs 0.2 expected (diff: 0.023930636048316967)\n",
            "1998: 0.2 => (0.1) 0.08420801 actual vs 0.1 expected (diff: 0.01579198837280274)\n",
            "1999: 0.1 => (0.0) 0.033813857 actual vs 0.0 expected (diff: -0.03381385654211044)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def build_lstm_layer(bp, depth_type, layer_index, raw_x, width):\n",
        "#     \"\"\"\n",
        "#     Build a single LSTM layer (Graves 2013); can be stacked, but send in sequential layer_indexes to scope properly.\n",
        "#     \"\"\"\n",
        "#     global init, noise_m, noise_s\n",
        "#     # Define variable names\n",
        "#     h_name = \"hidden-%s\" % layer_index  # Really the 'output' of the LSTM layer\n",
        "#     c_name = \"cell-%s\" % layer_index\n",
        "#     # raw_x is [input_size, 1]\n",
        "#     input_size = raw_x.get_shape()[0].value\n",
        "#     # Why so serious? Introduce a little anarchy. Upset the established order...\n",
        "#     x = raw_x + tf.random_normal(raw_x.get_shape(), noise_m, noise_s)\n",
        "\n",
        "#     with tf.variable_scope(\"lstm_layer_%s\" % layer_index):\n",
        "\n",
        "#         # Define shapes for all the weights/biases, limited to just this layer (not shared with other layers)\n",
        "#         # Sizes are 'input_size' when mapping x and 'width' otherwise\n",
        "#         W_xi = tf.get_variable(\"W_xi\", [width, input_size], initializer=init)\n",
        "#         W_hi = tf.get_variable(\"W_hi\", [width, width], initializer=init)\n",
        "#         W_ci = tf.get_variable(\"W_ci\", [width, width], initializer=init)\n",
        "#         b_i =  tf.get_variable(\"b_i\",  [width, 1], initializer=init)\n",
        "#         W_xf = tf.get_variable(\"W_xf\", [width, input_size], initializer=init)\n",
        "#         W_hf = tf.get_variable(\"W_hf\", [width, width], initializer=init)\n",
        "#         W_cf = tf.get_variable(\"W_cf\", [width, width], initializer=init)\n",
        "#         b_f =  tf.get_variable(\"b_f\",  [width, 1], initializer=init)\n",
        "#         W_xc = tf.get_variable(\"W_xc\", [width, input_size], initializer=init)\n",
        "#         W_hc = tf.get_variable(\"W_hc\", [width, width], initializer=init)\n",
        "#         b_c =  tf.get_variable(\"b_c\",  [width, 1], initializer=init)\n",
        "#         W_xo = tf.get_variable(\"W_xo\", [width, input_size], initializer=init)\n",
        "#         W_ho = tf.get_variable(\"W_ho\", [width, width], initializer=init)\n",
        "#         W_co = tf.get_variable(\"W_co\", [width, width], initializer=init)\n",
        "#         b_o =  tf.get_variable(\"b_o\",  [width, 1], initializer=init)\n",
        "\n",
        "#         # Retrieve the previous roll-depth's data, with starting random data if first roll-depth.\n",
        "#         h_past = bp.get_past_variable(h_name, np.float32(np.random.normal(m, s, [width, 1])))\n",
        "#         c_past = bp.get_past_variable(c_name, np.float32(np.random.normal(m, s, [width, 1])))\n",
        "\n",
        "#         # Build graph - looks almost like Alex Graves wrote it!\n",
        "#         i = tf.sigmoid(tf.matmul(W_xi, x) + tf.matmul(W_hi, h_past) + tf.matmul(W_ci, c_past) + b_i)\n",
        "#         f = tf.sigmoid(tf.matmul(W_xf, x) + tf.matmul(W_hf, h_past) + tf.matmul(W_cf, c_past) + b_f)\n",
        "#         c = bp.name_variable(c_name, tf.multiply(f, c_past) + tf.multiply(i, tf.tanh(tf.matmul(W_xc, x) + tf.matmul(W_hc, h_past) + b_c)))\n",
        "#         o = tf.sigmoid(tf.matmul(W_xo, x) + tf.matmul(W_ho, h_past) + tf.matmul(W_co, c) + b_o)\n",
        "#         h = bp.name_variable(h_name, tf.multiply(o, tf.tanh(c)))\n",
        "\n",
        "#     return [c, h]\n",
        "\n",
        "\n",
        "# def build_dual_lstm_frame(bp, depth_type):\n",
        "#     \"\"\"\n",
        "#     Build a dual-layer LSTM followed by standard sigmoid/linear mapping\n",
        "#     \"\"\"\n",
        "#     global init, input_dimensions, output_dimensions, batch_size, lstm_width\n",
        "\n",
        "#     # I/O DATA\n",
        "#     input_placeholder = tf.placeholder(tf.float32, shape=(input_dimensions, batch_size))\n",
        "#     output_placeholder = tf.placeholder(tf.float32, shape=(output_dimensions, batch_size))\n",
        "\n",
        "#     last_output = input_placeholder\n",
        "#     for layer_index in range(2):\n",
        "#         [_, h] = build_lstm_layer(bp, depth_type, layer_index, last_output, lstm_width)\n",
        "#         last_output = h\n",
        "\n",
        "#     W = tf.get_variable(\"W\", [1, lstm_width], initializer=init)\n",
        "#     b = tf.get_variable(\"b\", [1,1], initializer=init)\n",
        "#     output_result = tf.sigmoid(tf.matmul(W, last_output) + b)\n",
        "\n",
        "#     # return array of whatever you want, but I/O placeholders FIRST.\n",
        "#     return [input_placeholder, output_placeholder, output_result]\n",
        "\n",
        "\n",
        "# def build_single_lstm_frame(bp, depth_type):\n",
        "#     \"\"\"\n",
        "#     Build a dual-layer LSTM followed by standard sigmoid/linear mapping\n",
        "#     \"\"\"\n",
        "#     global init, input_dimensions, output_dimensions, batch_size, lstm_width\n",
        "\n",
        "#     # I/O DATA\n",
        "#     input_placeholder = tf.placeholder(tf.float32, shape=(input_dimensions, batch_size))\n",
        "#     output_placeholder = tf.placeholder(tf.float32, shape=(output_dimensions, batch_size))\n",
        "\n",
        "#     last_output = input_placeholder\n",
        "#     for layer_index in range(1):\n",
        "#         [o, _] = build_lstm_layer(bp, depth_type, layer_index, last_output, lstm_width)\n",
        "#         last_output = o\n",
        "\n",
        "#     W = tf.get_variable(\"W\", [1, lstm_width], initializer=init)\n",
        "#     b = tf.get_variable(\"b\", [1,1], initializer=init)\n",
        "#     output_result = tf.sigmoid(tf.matmul(W, last_output) + b)\n",
        "\n",
        "#     # return array of whatever you want, but I/O placeholders FIRST.\n",
        "#     return [input_placeholder, output_placeholder, output_result]\n"
      ],
      "metadata": {
        "id": "km6JjqEnkuS9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ligas interesantes"
      ],
      "metadata": {
        "id": "zS3WZKbyk7A3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sobre LSTM: http://turing.iimas.unam.mx/~ivanvladimir/slides/rpyaa/11_lstm.html#/18\n",
        "- Más sobre LSTM: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "- LSTM de 2003: https://arxiv.org/pdf/1308.0850.pdf"
      ],
      "metadata": {
        "id": "o9faxFT2k8Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h3eUDp31HHcM"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}